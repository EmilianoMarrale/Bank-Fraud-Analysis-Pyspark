{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:02:03.401198Z",
     "start_time": "2024-12-03T14:02:03.203601Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 15:02:06 WARN Utils: Your hostname, MacBook-Pro-di-Lorenzo-2.local resolves to a loopback address: 127.0.0.1; using 192.168.1.166 instead (on interface en0)\n",
      "24/12/03 15:02:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/03 15:02:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"myapp\").master('local[*]').getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:02:08.360209Z",
     "start_time": "2024-12-03T14:02:03.401842Z"
    }
   },
   "id": "edc09673dccc84ed",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('dataset/preprocessed_data/preprocessedData_SI.csv', header=True, inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:02:20.969987Z",
     "start_time": "2024-12-03T14:02:08.361070Z"
    }
   },
   "id": "bd287ec52140e890",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fraud_bool: double (nullable = true)\n",
      " |-- income: double (nullable = true)\n",
      " |-- name_email_similarity: double (nullable = true)\n",
      " |-- prev_address_months_count: double (nullable = true)\n",
      " |-- current_address_months_count: double (nullable = true)\n",
      " |-- customer_age: double (nullable = true)\n",
      " |-- days_since_request: double (nullable = true)\n",
      " |-- intended_balcon_amount: double (nullable = true)\n",
      " |-- zip_count_4w: double (nullable = true)\n",
      " |-- velocity_6h: double (nullable = true)\n",
      " |-- velocity_24h: double (nullable = true)\n",
      " |-- velocity_4w: double (nullable = true)\n",
      " |-- bank_branch_count_8w: double (nullable = true)\n",
      " |-- date_of_birth_distinct_emails_4w: double (nullable = true)\n",
      " |-- credit_risk_score: double (nullable = true)\n",
      " |-- email_is_free: double (nullable = true)\n",
      " |-- phone_home_valid: double (nullable = true)\n",
      " |-- phone_mobile_valid: double (nullable = true)\n",
      " |-- bank_months_count: double (nullable = true)\n",
      " |-- has_other_cards: double (nullable = true)\n",
      " |-- proposed_credit_limit: double (nullable = true)\n",
      " |-- foreign_request: double (nullable = true)\n",
      " |-- session_length_in_minutes: double (nullable = true)\n",
      " |-- keep_alive_session: double (nullable = true)\n",
      " |-- device_distinct_emails_8w: double (nullable = true)\n",
      " |-- month_encoded: integer (nullable = true)\n",
      " |-- payment_type_encoded: double (nullable = true)\n",
      " |-- employment_status_encoded: double (nullable = true)\n",
      " |-- housing_status_encoded: double (nullable = true)\n",
      " |-- source_encoded: double (nullable = true)\n",
      " |-- device_os_encoded: double (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:02:20.986766Z",
     "start_time": "2024-12-03T14:02:20.970717Z"
    }
   },
   "id": "117eff7d9107c9f3",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 15:02:23 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "1000000"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:02:23.545228Z",
     "start_time": "2024-12-03T14:02:20.987395Z"
    }
   },
   "id": "61fbd5526a8d33b0",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Rename target_bool column to label\n",
    "df_transformed = df.withColumnRenamed(\"fraud_bool\", \"label\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:02:23.588019Z",
     "start_time": "2024-12-03T14:02:23.546065Z"
    }
   },
   "id": "31cb99e17de42afb",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification with GBTClassifier and base dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bff1591ccdfaac7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get a list of the columns in the dataset\n",
    "columns = df_transformed.columns\n",
    "\n",
    "# Remove the target column from the list\n",
    "columns.remove(\"label\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:07:18.604843Z",
     "start_time": "2024-12-03T14:07:18.602504Z"
    }
   },
   "id": "673483e197ea5e16",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "data_transformed = assembler.transform(df_transformed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:07:19.218059Z",
     "start_time": "2024-12-03T14:07:18.992131Z"
    }
   },
   "id": "266da5e15ee8b6b4",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Prepare the final dataset\n",
    "dataset = data_transformed.select('features', 'label')  # Ensure you have 'features' and 'label' columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:07:19.425377Z",
     "start_time": "2024-12-03T14:07:19.398795Z"
    }
   },
   "id": "3c8a5fc5947224e3",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data, test_data = dataset.randomSplit([0.8, 0.2], seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:07:19.833721Z",
     "start_time": "2024-12-03T14:07:19.817310Z"
    }
   },
   "id": "8d71da6a70ac0349",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the baseline model and no class imbalance handling or hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5ac35cd8c37c745"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gbt_baseline = GBTClassifier(labelCol=\"label\", seed=42, featuresCol=\"features\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T00:25:16.573020Z",
     "start_time": "2024-12-03T00:25:16.535386Z"
    }
   },
   "id": "75197428fe1b724e",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_1 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_7 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_4 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_8 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_3 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_9 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_11 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_0 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_10 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_5 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_6 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 01:25:42 WARN MemoryStore: Not enough space to cache rdd_37_2 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_2 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_5 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_9 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_1 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_11 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_10 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_3 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_4 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_6 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_8 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_0 to disk instead.\n",
      "24/12/03 01:25:43 WARN BlockManager: Persisting block rdd_37_7 to disk instead.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "baseline_model = gbt_baseline.fit(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T00:28:15.898594Z",
     "start_time": "2024-12-03T00:25:31.067277Z"
    }
   },
   "id": "eef1ab2a30518064",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "baseline_predictions = baseline_model.transform(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T00:29:40.858873Z",
     "start_time": "2024-12-03T00:29:40.559902Z"
    }
   },
   "id": "13e9def5a17b174a",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.86\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Calculate accuracy\n",
    "auc_baseline = evaluator.evaluate(baseline_predictions)\n",
    "print(f\"AUC = {auc_baseline:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T00:30:04.011729Z",
     "start_time": "2024-12-03T00:29:52.905325Z"
    }
   },
   "id": "43f74dbed644c771",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 446:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative              197501                   3\n",
      "Actual Positive                2189                   7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results = baseline_predictions.withColumn(\"prediction\", F.col(\"prediction\").cast(\"double\"))\n",
    "\n",
    "tp = results.filter((F.col(\"label\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "tn = results.filter((F.col(\"label\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "fp = results.filter((F.col(\"label\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "fn = results.filter((F.col(\"label\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "confusion_matrix_baselinemodel = pd.DataFrame(\n",
    "    [[tn, fp], [fn, tp]],\n",
    "    index=[\"Actual Negative\", \"Actual Positive\"],\n",
    "    columns=[\"Predicted Negative\", \"Predicted Positive\"]\n",
    ")\n",
    "\n",
    "print(confusion_matrix_baselinemodel)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T00:32:22.658812Z",
     "start_time": "2024-12-03T00:31:59.263443Z"
    }
   },
   "id": "83f6dd9005c2d29a",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.00\n",
      "Precision: 0.70\n",
      "F1 Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Recall, precision and f1-score\n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T00:31:11.989313Z",
     "start_time": "2024-12-03T00:31:11.985583Z"
    }
   },
   "id": "b4d85b4e8a4da5cd",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the model is totally biased towards the majority class. This is because the dataset is imbalanced. We can handle this by using weights or by undersampling the majority class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8a9178ed8ed8366"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using weights to handle class imbalance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "310689a836153b3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_count = df_transformed.count()\n",
    "pos_count = df_transformed.filter(col(\"label\") == 1).count()\n",
    "neg_count = total_count - pos_count\n",
    "\n",
    "# Compute class weights\n",
    "pos_weight = total_count / (2.0 * pos_count)\n",
    "neg_weight = total_count / (2.0 * neg_count)\n",
    "\n",
    "# Add the weight column to the dataset\n",
    "df_withWeights = df_transformed.withColumn(\n",
    "    \"weight\", when(col(\"label\") == 1, pos_weight).otherwise(neg_weight)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T01:04:14.541546Z",
     "start_time": "2024-12-03T01:04:10.721514Z"
    }
   },
   "id": "a99c14f8b6fa553d",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns = df_withWeights.columns\n",
    "\n",
    "# Remove the target column and weight column from the list of feature columns\n",
    "columns.remove(\"label\")\n",
    "columns.remove(\"weight\")\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "data_transformed = assembler.transform(df_withWeights)\n",
    "\n",
    "# Prepare the final dataset\n",
    "dataset = data_transformed.select(\"features\", \"label\", \"weight\")  # Include 'weight' column\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_data, test_data = dataset.randomSplit([0.8, 0.2], seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T01:04:50.615837Z",
     "start_time": "2024-12-03T01:04:50.315961Z"
    }
   },
   "id": "409d1da1d05b2b72",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_11 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_6 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_1 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_5 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_0 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_3 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_7 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_8 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_10 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_9 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_4 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 02:05:26 WARN MemoryStore: Not enough space to cache rdd_1077_2 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_6 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_10 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_9 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_5 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_11 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_2 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_4 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_0 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_3 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_7 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_8 to disk instead.\n",
      "24/12/03 02:05:26 WARN BlockManager: Persisting block rdd_1077_1 to disk instead.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.8855868299215742\n"
     ]
    }
   ],
   "source": [
    "gbt_withWeights = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\", maxIter=30, maxDepth=3, stepSize=0.3, seed=42)\n",
    "\n",
    "model_withWeights = gbt_withWeights.fit(train_data)\n",
    "\n",
    "# We will not execute it because we already did it\n",
    "'''\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#     .addGrid(gbt_withWeights.maxDepth, [3, 5, 7]) \\\n",
    "#     .addGrid(gbt_withWeights.maxIter, [10, 20, 30]) \\\n",
    "#     .addGrid(gbt_withWeights.stepSize, [0.1, 0.2, 0.3]) \\\n",
    "#     .build()\n",
    "# \n",
    "# evaluator = BinaryClassificationEvaluator(\n",
    "#     labelCol=\"label\",\n",
    "#     rawPredictionCol=\"rawPrediction\",\n",
    "#     metricName=\"areaUnderROC\"  # Use AUC as the metric\n",
    "# )\n",
    "# \n",
    "# crossval = CrossValidator(\n",
    "#     estimator=gbt_withWeights,\n",
    "#     estimatorParamMaps=paramGrid,\n",
    "#     evaluator=evaluator,\n",
    "#     numFolds=3,  # Use 3-fold cross-validation\n",
    "#     parallelism=2  # Number of threads to use\n",
    "# )\n",
    "# \n",
    "# cvModel = crossval.fit(train_data)\n",
    "# \n",
    "# bestModel = cvModel.bestModel\n",
    "# print(f\"Best maxDepth: {bestModel.getMaxDepth()}\")\n",
    "# print(f\"Best maxIter: {bestModel.getMaxIter()}\")\n",
    "# print(f\"Best stepSize: {bestModel.getStepSize()}\")\n",
    "'''\n",
    "\n",
    "predictions_withWeights = model_withWeights.transform(test_data)\n",
    "roc_auc_weights = evaluator.evaluate(predictions_withWeights)\n",
    "print(f\"Test AUC: {roc_auc_weights}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T01:06:32.214428Z",
     "start_time": "2024-12-03T01:05:02.145772Z"
    }
   },
   "id": "fefeeb2728423621",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 671:====>                                                  (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative              160007               37497\n",
      "Actual Positive                 441                1755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results = predictions_withWeights.withColumn(\"prediction\", F.col(\"prediction\").cast(\"double\"))\n",
    "\n",
    "tp = results.filter((F.col(\"label\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "tn = results.filter((F.col(\"label\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "fp = results.filter((F.col(\"label\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "fn = results.filter((F.col(\"label\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "confusion_matrix_weighted = pd.DataFrame(\n",
    "    [[tn, fp], [fn, tp]],\n",
    "    index=[\"Actual Negative\", \"Actual Positive\"],\n",
    "    columns=[\"Predicted Negative\", \"Predicted Positive\"]\n",
    ")\n",
    "\n",
    "print(confusion_matrix_weighted)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T01:08:41.540606Z",
     "start_time": "2024-12-03T01:08:08.654285Z"
    }
   },
   "id": "b4154c50ee7eb3d9",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.80\n",
      "Precision: 0.04\n",
      "F1 Score: 0.08\n"
     ]
    }
   ],
   "source": [
    "# Recall, precision and f1-score\n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T01:08:54.380088Z",
     "start_time": "2024-12-03T01:08:54.376614Z"
    }
   },
   "id": "765441a2fed7f989",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training model with random undersampling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c57c27b3408c5f1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_11 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_3 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_6 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_10 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_8 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_0 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_1 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_5 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_2 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_7 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_9 in memory! (computed 7.9 MiB so far)\n",
      "24/12/03 15:08:17 WARN MemoryStore: Not enough space to cache rdd_645_4 in memory! (computed 5.2 MiB so far)\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_2 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_3 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_9 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_8 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_10 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_5 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_7 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_11 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_0 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_4 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_1 to disk instead.\n",
      "24/12/03 15:08:17 WARN BlockManager: Persisting block rdd_645_6 to disk instead.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|11122|\n",
      "|  1.0|11029|\n",
      "+-----+-----+\n"
     ]
    }
   ],
   "source": [
    "# Under-sampling the majority class\n",
    "majority_class = df_transformed.filter(df_transformed['label'] == 0)\n",
    "minority_class = df_transformed.filter(df_transformed['label'] == 1)\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "seed = 42\n",
    "# Downsample the majority class\n",
    "majority_downsampled = majority_class.sample(False, minority_class.count() / majority_class.count(), seed)\n",
    "\n",
    "# Combine the downsampled majority class with the minority class\n",
    "df_undersampled = majority_downsampled.union(minority_class)\n",
    "\n",
    "# Show the number of fraud and non-fraud transactions\n",
    "df_undersampled.groupBy('label').count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:08:34.554466Z",
     "start_time": "2024-12-03T14:08:08.982825Z"
    }
   },
   "id": "bf36664b472ed61c",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "data_transformed = assembler.transform(df_undersampled)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-12-03T14:08:34.555477Z"
    }
   },
   "id": "7c1ee76721bea09",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Prepare the final dataset\n",
    "dataset = data_transformed.select('features', 'label')  # Ensure you have 'features' and 'label' columns"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "11532af574844e90",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data, test_data = dataset.randomSplit([0.8, 0.2], seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "648b8a1f7ca2877e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Declare base gbt model for cv\n",
    "gbt_undersampling = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=30, maxDepth=3, stepSize=0.3, seed=42)\n",
    "'''\n",
    "# We will not execute it because we already did it\n",
    "#paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [3, 5, 7]) \\\n",
    "    .addGrid(gbt.maxIter, [10, 20, 30]) \\\n",
    "    .addGrid(gbt.stepSize, [0.1, 0.2, 0.3]) \\\n",
    "    .build()\n",
    "\n",
    "# Step 3: Set up the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"  # Use AUC as the metric\n",
    ")\n",
    "\n",
    "# Step 4: Set up CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator=gbt,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,  # Use 3-fold cross-validation\n",
    "    parallelism=2  # Number of threads to use\n",
    ")\n",
    "# Step 5: Train the model with cross-validation\n",
    "\n",
    "# Step 6: Extract the best model and parameters\n",
    "bestModel = cvModel.bestModel\n",
    "print(f\"Best maxDepth: {bestModel.getMaxDepth()}\")\n",
    "print(f\"Best maxIter: {bestModel.getMaxIter()}\")\n",
    "print(f\"Best stepSize: {bestModel.getStepSize()}\")\n",
    "'''\n",
    "\n",
    "model_undersampling = gbt_undersampling.fit(train_data)\n",
    "\n",
    "predictions_undersampling = model_undersampling.transform(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "undersampled_auc = evaluator.evaluate(predictions_undersampling)\n",
    "\n",
    "print(f\"Test AUC: {undersampled_auc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "49a9cd78957180",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Same hyperparameters as before but with random undersampling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28f505127c5f9a67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results = predictions_undersampling.withColumn(\"prediction\", F.col(\"prediction\").cast(\"double\"))\n",
    "\n",
    "tp = results.filter((F.col(\"label\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "tn = results.filter((F.col(\"label\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "fp = results.filter((F.col(\"label\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "fn = results.filter((F.col(\"label\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "confusion_matrix_undersampling = pd.DataFrame(\n",
    "    [[tn, fp], [fn, tp]],\n",
    "    index=[\"Actual Negative\", \"Actual Positive\"],\n",
    "    columns=[\"Predicted Negative\", \"Predicted Positive\"]\n",
    ")\n",
    "\n",
    "print(confusion_matrix_undersampling)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d6cb6854aec506c2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Recall, precision and f1-score\n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n",
    "\n",
    "#Accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "522ea4f0990d17e9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 12) / 12]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m model_undersampling \u001B[38;5;241m=\u001B[39m \u001B[43mgbt_undersampling\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Make predictions\u001B[39;00m\n\u001B[1;32m      5\u001B[0m predictions_undersampling \u001B[38;5;241m=\u001B[39m model_undersampling\u001B[38;5;241m.\u001B[39mtransform(test_data)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 381\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions_undersampling = model_undersampling.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "undersampled_auc = evaluator.evaluate(predictions_undersampling)\n",
    "\n",
    "print(f\"Test AUC: {undersampled_auc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:08:03.396603Z",
     "start_time": "2024-12-03T14:07:55.862350Z"
    }
   },
   "id": "26fae6d46d8d7e13",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Features importance\n",
    "feature_importance = model_undersampling.featureImportances\n",
    "\n",
    "# print the feature importance of each feature\n",
    "for i, feature in enumerate(columns):\n",
    "    print(f\"{feature}: {feature_importance[i]}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee728a3f6097814c",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
